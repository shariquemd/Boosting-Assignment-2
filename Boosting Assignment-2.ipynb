{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c6937-d53c-47e2-9c20-2006655076c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "Gradient Boosting Regression is a machine learning algorithm that builds a predictive model in the form of an ensemble of weak learners, typically decision trees. It sequentially trains these weak learners, with each subsequent model correcting the errors of the previous ones. The process involves minimizing a loss function, often the mean squared error for regression problems, by adjusting the predictions at each step.\n",
    "\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a sample dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X.squeeze() + np.random.randn(100)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize parameters\n",
    "learning_rate = 0.1\n",
    "n_trees = 100\n",
    "tree_depth = 3\n",
    "\n",
    "# Initialize the prediction with the mean of the target variable\n",
    "predictions = np.full_like(y_train, fill_value=np.mean(y_train))\n",
    "\n",
    "# Gradient boosting training\n",
    "for i in range(n_trees):\n",
    "    # Calculate the negative gradient (residuals)\n",
    "    residuals = y_train - predictions\n",
    "    \n",
    "    # Fit a weak learner (decision tree) to the negative gradient\n",
    "    tree = DecisionTreeRegressor(max_depth=tree_depth)\n",
    "    tree.fit(X_train, residuals)\n",
    "    \n",
    "    # Update predictions by adding the scaled weak learner\n",
    "    predictions += learning_rate * tree.predict(X_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = np.full_like(y_test, fill_value=np.mean(y_train))\n",
    "for i in range(n_trees):\n",
    "    y_pred += learning_rate * tree.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "\n",
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimize the performance of the model. Use grid search or random search to find the best hyperparameters.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_trees': [50, 100, 150],\n",
    "    'tree_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create a gradient boosting model\n",
    "def create_gb_model(learning_rate, n_trees, tree_depth):\n",
    "    model = []\n",
    "    predictions = np.full_like(y_train, fill_value=np.mean(y_train))\n",
    "    for _ in range(n_trees):\n",
    "        residuals = y_train - predictions\n",
    "        tree = DecisionTreeRegressor(max_depth=tree_depth)\n",
    "        tree.fit(X_train, residuals)\n",
    "        predictions += learning_rate * tree.predict(X_train)\n",
    "        model.append(tree)\n",
    "    return model\n",
    "\n",
    "# Create the model for use in GridSearchCV\n",
    "gb_model = create_gb_model(learning_rate=0.1, n_trees=100, tree_depth=3)\n",
    "\n",
    "# Define the GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the model to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Evaluate the model with the best hyperparameters on the test set\n",
    "best_gb_model = create_gb_model(**best_params)\n",
    "y_pred_best = np.full_like(y_test, fill_value=np.mean(y_train))\n",
    "for tree in best_gb_model:\n",
    "    y_pred_best += learning_rate * tree.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Best Mean Squared Error: {mse_best}\")\n",
    "print(f\"Best R-squared: {r2_best}\")\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "A weak learner in Gradient Boosting is a model that performs slightly better than random chance. In the context of decision trees, weak learners are often shallow trees with a limited number of nodes.\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The intuition is to sequentially add models to the ensemble, each correcting the errors made by the existing ensemble. The algorithm focuses on the mistakes of previous models, learning from them and improving overall predictive accuracy.\n",
    "Q6. How does the Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "It starts with an initial weak learner and iteratively adds more models, each trained to correct the errors of the existing ensemble. The weights of the weak learners are adjusted based on their performance.\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm?\n",
    "\n",
    "The key steps involve defining a loss function, initializing the model with a simple constant prediction, computing the negative gradient of the loss function, fitting a weak learner to the negative gradient, and updating the model by adding a scaled version of the new weak learner. This process repeats until a predefined number of iterations or until convergence. Would you like more details?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
